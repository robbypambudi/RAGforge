{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T03:39:44.008268Z",
     "start_time": "2025-06-22T03:39:40.857609Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from loguru import logger\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "class ImprovedDocumentCleaner:\n",
    "    def __init__(self, preserve_context=True):\n",
    "        \"\"\"\n",
    "        Initialize document cleaner optimized for Indonesian RAG systems\n",
    "\n",
    "        Args:\n",
    "            preserve_context: If True, preserves important contextual information\n",
    "        \"\"\"\n",
    "        self.preserve_context = preserve_context\n",
    "\n",
    "        # Indonesian stopwords yang aman untuk dihapus (tidak menghilangkan konteks penting)\n",
    "        # Mengurangi daftar stopwords untuk preservasi konteks\n",
    "        self.safe_stopwords = {\n",
    "            'adalah', 'akan', 'atau', 'belum', 'bisa', 'dapat', 'harus',\n",
    "            'juga', 'karena', 'ketika', 'maka', 'namun', 'saat', 'sangat',\n",
    "            'saya', 'sebagai', 'sebuah', 'sudah', 'telah', 'tersebut',\n",
    "            'tetapi', 'tidak', 'untuk', 'yang', 'yaitu'\n",
    "        }\n",
    "\n",
    "        # Stopwords berbahaya yang TIDAK boleh dihapus untuk konteks KP\n",
    "        self.important_contextual_words = {\n",
    "            'di', 'ke', 'dari', 'dengan', 'pada', 'dalam', 'melalui', 'via',\n",
    "            'kepada', 'oleh', 'setelah', 'sebelum', 'sampai', 'hingga'\n",
    "        }\n",
    "\n",
    "        logger.info('ImprovedDocumentCleaner initialized for Indonesian RAG system.')\n",
    "\n",
    "    def normalize_unicode(self, text: str) -> str:\n",
    "        \"\"\"Normalize Unicode characters while preserving Indonesian characters\"\"\"\n",
    "        # Preserve common Indonesian characters\n",
    "        text = text.replace('–', '-').replace('—', '-')  # Normalize dashes\n",
    "        text = text.replace('\"', '\"').replace('\"', '\"')  # Normalize quotes\n",
    "        text = text.replace(''', \"'\").replace(''', \"'\")  # Normalize apostrophes\n",
    "\n",
    "        # Only normalize problematic Unicode, not all\n",
    "        text = re.sub(r'[\\u200b\\u200c\\u200d\\ufeff]', '', text)  # Remove zero-width chars\n",
    "        return text\n",
    "\n",
    "    def remove_headers_footers(self, text: str) -> str:\n",
    "        \"\"\"Conservative header and footer removal\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "\n",
    "        # Only remove obvious headers/footers, not content\n",
    "        header_patterns = [\n",
    "            r'^\\s*page\\s+\\d+\\s*$',\n",
    "            r'^\\s*\\d+\\s*$',  # Just page numbers\n",
    "            r'^\\s*confidential\\s*$',\n",
    "            r'^\\s*draft\\s*$'\n",
    "        ]\n",
    "\n",
    "        footer_patterns = [\n",
    "            r'^\\s*copyright.*$',\n",
    "            r'^\\s*all rights reserved.*$',\n",
    "            r'^\\s*-\\s*\\d+\\s*-\\s*$'\n",
    "        ]\n",
    "\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            is_header_footer = any(\n",
    "                re.match(pattern, line.strip(), re.IGNORECASE)\n",
    "                for pattern in header_patterns + footer_patterns\n",
    "            )\n",
    "\n",
    "            if not is_header_footer and line.strip():\n",
    "                cleaned_lines.append(line)\n",
    "\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "    def preserve_important_patterns(self, text: str) -> str:\n",
    "        \"\"\"Preserve patterns important for KP domain\"\"\"\n",
    "        # Preserve email patterns\n",
    "        text = re.sub(r'(\\w+)\\s*\\.\\s*(\\w+)\\s*@\\s*(\\w+)\\s*\\.\\s*(\\w+)\\s*\\.\\s*(\\w+)',\n",
    "                      r'\\1.\\2@\\3.\\4.\\5', text)\n",
    "\n",
    "        # Preserve phone numbers\n",
    "        text = re.sub(r'(\\+?\\s*\\d{2,3})\\s*(\\d{3,4})\\s*-?\\s*(\\d{4,5})\\s*-?\\s*(\\d{4})',\n",
    "                      r'\\1 \\2-\\3-\\4', text)\n",
    "\n",
    "        # Preserve academic codes (like course codes)\n",
    "        text = re.sub(r'([A-Z]+)\\s*(\\d+)', r'\\1\\2', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def clean_special_characters(self, text: str) -> str:\n",
    "        \"\"\"Conservative special character cleaning\"\"\"\n",
    "        # Only remove truly problematic characters\n",
    "        # Keep: . , ! ? : ; - ( ) [ ] / @\n",
    "        problematic_chars = ['*', '\\\\', '|', '#', '$', '%', '^', '&', '~', '`']\n",
    "\n",
    "        for char in problematic_chars:\n",
    "            text = text.replace(char, ' ')\n",
    "\n",
    "        # Clean up multiple spaces but preserve structure\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs to single space\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Multiple newlines to double newline\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def normalize_indonesian_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize Indonesian text patterns\"\"\"\n",
    "        # Standardize common Indonesian abbreviations\n",
    "        abbreviations = {\n",
    "            r'\\bdr\\b': 'doktor',\n",
    "            r'\\bprof\\b': 'profesor',\n",
    "            r'\\bkp\\b': 'kerja praktik',\n",
    "            r'\\bmhs\\b': 'mahasiswa',\n",
    "            r'\\bdgn\\b': 'dengan',\n",
    "            r'\\butk\\b': 'untuk',\n",
    "            r'\\byg\\b': 'yang',\n",
    "            r'\\btsb\\b': 'tersebut',\n",
    "            r'\\bdll\\b': 'dan lain lain'\n",
    "        }\n",
    "\n",
    "        for abbrev, full_form in abbreviations.items():\n",
    "            text = re.sub(abbrev, full_form, text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def smart_stopword_removal(self, text: str) -> str:\n",
    "        \"\"\"Smart stopword removal that preserves context\"\"\"\n",
    "        if not self.preserve_context:\n",
    "            return text\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        cleaned_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "\n",
    "            # Only remove safe stopwords, keep contextual ones\n",
    "            filtered_words = []\n",
    "            for i, word in enumerate(words):\n",
    "                # Always keep important contextual words\n",
    "                if word in self.important_contextual_words:\n",
    "                    filtered_words.append(word)\n",
    "                # Remove safe stopwords only if they don't break context\n",
    "                elif word in self.safe_stopwords:\n",
    "                    # Keep if it's part of important phrase\n",
    "                    if i > 0 and i < len(words) - 1:\n",
    "                        prev_word = words[i - 1]\n",
    "                        next_word = words[i + 1]\n",
    "                        # Keep if surrounded by important words\n",
    "                        if (prev_word not in self.safe_stopwords or\n",
    "                                next_word not in self.safe_stopwords):\n",
    "                            filtered_words.append(word)\n",
    "                else:\n",
    "                    filtered_words.append(word)\n",
    "\n",
    "            if filtered_words:\n",
    "                cleaned_sentences.append(' '.join(filtered_words))\n",
    "\n",
    "        return ' '.join(cleaned_sentences)\n",
    "\n",
    "    def minimal_clean(self, text: str) -> str:\n",
    "        \"\"\"Minimal cleaning that preserves semantic meaning\"\"\"\n",
    "        # Step 1: Basic normalization\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.preserve_important_patterns(text)\n",
    "\n",
    "        # Step 2: Conservative structure cleaning\n",
    "        text = self.remove_headers_footers(text)\n",
    "        text = self.clean_special_characters(text)\n",
    "\n",
    "        # Step 3: Text normalization\n",
    "        text = self.normalize_indonesian_text(text)\n",
    "\n",
    "        # Step 4: Final cleanup\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def standard_clean(self, text: str) -> str:\n",
    "        \"\"\"Standard cleaning with smart preprocessing\"\"\"\n",
    "        # Start with minimal clean\n",
    "        text = self.minimal_clean(text)\n",
    "\n",
    "        # Add smart stopword removal\n",
    "        text = self.smart_stopword_removal(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def aggressive_clean(self, text: str) -> str:\n",
    "        \"\"\"Aggressive cleaning (use with caution for RAG)\"\"\"\n",
    "        text = self.standard_clean(text)\n",
    "\n",
    "        # Additional aggressive steps\n",
    "        # Remove very common but less meaningful words\n",
    "        aggressive_stopwords = {\n",
    "            'ini', 'itu', 'disini', 'disitu', 'begitu', 'begini',\n",
    "            'demikian', 'seperti', 'misalnya', 'contoh', 'yaitu'\n",
    "        }\n",
    "\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word.lower() not in aggressive_stopwords]\n",
    "\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def clean_document(self, text: str, level: str = \"minimal\") -> str:\n",
    "        \"\"\"\n",
    "        Main cleaning pipeline with different levels\n",
    "\n",
    "        Args:\n",
    "            text: Input text to clean\n",
    "            level: \"minimal\", \"standard\", or \"aggressive\"\n",
    "\n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        if level == \"minimal\":\n",
    "            return self.minimal_clean(text)\n",
    "        elif level == \"standard\":\n",
    "            return self.standard_clean(text)\n",
    "        elif level == \"aggressive\":\n",
    "            return self.aggressive_clean(text)\n",
    "        else:\n",
    "            raise ValueError(\"Level must be 'minimal', 'standard', or 'aggressive'\")\n",
    "\n",
    "\n",
    "# Comparison function untuk testing\n",
    "def compare_cleaning_levels(original_text: str):\n",
    "    \"\"\"Compare different cleaning levels\"\"\"\n",
    "    cleaner = ImprovedDocumentCleaner()\n",
    "\n",
    "    results = {\n",
    "        \"original\": original_text,\n",
    "        \"minimal\": cleaner.clean_document(original_text, \"minimal\"),\n",
    "        \"standard\": cleaner.clean_document(original_text, \"standard\"),\n",
    "        \"aggressive\": cleaner.clean_document(original_text, \"aggressive\")\n",
    "    }\n",
    "\n",
    "    print(\"=== CLEANING COMPARISON ===\")\n",
    "    for level, text in results.items():\n",
    "        print(f\"\\n{level.upper()}:\")\n",
    "        print(f\"Length: {len(text)} chars\")\n",
    "        print(f\"Text: {text[:200]}...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Test text from your KP domain\n",
    "    test_text = \"\"\"\n",
    "    kerja praktik reguler ( kelas non - alih kredit ) prosedur kp 1. mengisi form pengajuan kp ( bisa cek di link dokumen - dokumen kp ). jika kp dilakukan internal di if, silakan minta surat keterangan kp ( format ada di link dokumen - dokumen kp ) dari dosen pembimbing kp anda dan langsung ke step nomor 5. jika kp yang diikuti adalah lab. internship luar negeri dapat menggunakan loa dan langsung ke step nomor 5. 2. mengajukan form pengajuan kp via email ke hadziq its. ac. id cc inka. nd its. ac. id dengan format pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compare different levels\n",
    "    compare_cleaning_levels(test_text)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-06-22 10:39:43.996\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m39\u001B[0m - \u001B[1mImprovedDocumentCleaner initialized for Indonesian RAG system.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING COMPARISON ===\n",
      "\n",
      "ORIGINAL:\n",
      "Length: 530 chars\n",
      "Text: \n",
      "    kerja praktik reguler ( kelas non - alih kredit ) prosedur kp 1. mengisi form pengajuan kp ( bisa cek di link dokumen - dokumen kp ). jika kp dilakukan internal di if, silakan minta surat keteran...\n",
      "--------------------------------------------------\n",
      "\n",
      "MINIMAL:\n",
      "Length: 619 chars\n",
      "Text: kerja praktik reguler ( kelas non - alih kredit ) prosedur kerja praktik 1. mengisi form pengajuan kerja praktik ( bisa cek di link dokumen - dokumen kerja praktik ). jika kerja praktik dilakukan inte...\n",
      "--------------------------------------------------\n",
      "\n",
      "STANDARD:\n",
      "Length: 629 chars\n",
      "Text: kerja praktik reguler ( kelas non - alih kredit ) prosedur kerja praktik 1. mengisi form pengajuan kerja praktik ( bisa cek di link dokumen - dokumen kerja praktik ) . jika kerja praktik dilakukan int...\n",
      "--------------------------------------------------\n",
      "\n",
      "AGGRESSIVE:\n",
      "Length: 629 chars\n",
      "Text: kerja praktik reguler ( kelas non - alih kredit ) prosedur kerja praktik 1. mengisi form pengajuan kerja praktik ( bisa cek di link dokumen - dokumen kerja praktik ) . jika kerja praktik dilakukan int...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d2927b09ec7b8ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
